{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is our jupyter notebook file where we train our agent on the starpilot environment with the optimal hyperparameters we found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE: Because of the script's dependence on cuda, this notebook will most likely only be able to run in DTU's hpc cluster or in google colab."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we import the necessary libraries. You might need to install them with pip3 first and also add the local utils file so the script can import it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#!pip3 install procgen\n",
    "from utils import make_env, Storage, orthogonal_init\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we define the hyperparameters: NOTE: if you are in colab you have to take less num_steps because of ram limit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_steps = 8e6  # unchangeable\n",
    "num_envs = 64  # unchangeable\n",
    "num_levels = 10000\n",
    "num_steps = 256\n",
    "num_epochs = 3\n",
    "batch_size = 16\n",
    "eps = .2\n",
    "grad_eps = .5\n",
    "value_coef = .5\n",
    "entropy_coef = .01\n",
    "learning_rate = 1e-4\n",
    "optimizer_eps = 1e-5\n",
    "feature_dim = 16\n",
    "in_channels = 3  # unchangeable"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we define the model and environment."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, feature_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
    "        )\n",
    "        self.apply(orthogonal_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, encoder, feature_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
    "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
    "\n",
    "    def act(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda().contiguous()\n",
    "            dist, value = self.forward(x)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        logits = self.policy(x)\n",
    "        value = self.value(x).squeeze(1)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        return dist, value\n",
    "\n",
    "\n",
    "# Define environment\n",
    "# check the utils.py file for info on arguments\n",
    "env = make_env(num_envs, num_levels=num_levels)\n",
    "print('Observation space:', env.observation_space)\n",
    "print('Action space:', env.action_space.n)\n",
    "\n",
    "eval_env = make_env(num_envs, num_levels=num_levels, seed=2)\n",
    "eval_obs = eval_env.reset()\n",
    "\n",
    "# Define network\n",
    "num_actions = env.action_space.n\n",
    "encoder = Encoder(in_channels, feature_dim)\n",
    "policy = Policy(encoder, feature_dim, num_actions)\n",
    "policy.cuda()\n",
    "\n",
    "# Define optimizer\n",
    "# these are reasonable values but probably not optimal\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate, eps=optimizer_eps)\n",
    "\n",
    "# Define temporary storage\n",
    "# we use this to collect transitions during each iteration\n",
    "storage = Storage(\n",
    "    env.observation_space.shape,\n",
    "    num_steps,\n",
    "    num_envs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the training part."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run training\n",
    "obs = env.reset()\n",
    "step = 0\n",
    "\n",
    "data_steps = []\n",
    "data_val = []\n",
    "data_train = []\n",
    "\n",
    "while step < total_steps:\n",
    "\n",
    "    # Use policy to collect data for num_steps steps\n",
    "    policy.eval()\n",
    "    for _ in range(num_steps):\n",
    "        # Use policy\n",
    "        action, log_prob, value = policy.act(obs)\n",
    "\n",
    "        # Take step in environment\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # print(reward.mean())\n",
    "        # Store data\n",
    "        storage.store(obs, action, reward, done, info, log_prob, value)\n",
    "\n",
    "        # Update current observation\n",
    "        obs = next_obs\n",
    "\n",
    "    # Add the last observation to collected data\n",
    "    _, _, value = policy.act(obs)\n",
    "    storage.store_last(obs, value)\n",
    "\n",
    "    # Compute return and advantage\n",
    "    storage.compute_return_advantage()\n",
    "\n",
    "    # Optimize policy\n",
    "    policy.train()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Iterate over batches of transitions\n",
    "        generator = storage.get_generator(batch_size)\n",
    "        for batch in generator:\n",
    "            b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
    "\n",
    "            # Get current policy outputs\n",
    "            new_dist, new_value = policy(b_obs)\n",
    "            new_log_prob = new_dist.log_prob(b_action)\n",
    "\n",
    "            # Clipped policy objective\n",
    "            ratio = torch.exp(new_log_prob - b_log_prob)\n",
    "            clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps)\n",
    "            policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n",
    "            pi_loss = policy_reward.mean()\n",
    "\n",
    "            # Clipped value function objective\n",
    "            clipped_value = b_value + (new_value - b_value).clamp(min=-eps, max=eps)\n",
    "            vf_loss = torch.max((new_value - b_returns) ** 2, (clipped_value - b_returns) ** 2)\n",
    "            value_loss = 0.5 * vf_loss.mean()\n",
    "\n",
    "            # Entropy loss\n",
    "            entropy_loss = new_dist.entropy().mean()\n",
    "\n",
    "            # Backpropagate losses\n",
    "            c_1 = value_coef\n",
    "            c_2 = entropy_coef\n",
    "            loss = -(pi_loss - c_1 * value_loss + c_2 * entropy_loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
    "\n",
    "            # Update policy\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # Update stats\n",
    "    step += num_envs * num_steps\n",
    "\n",
    "    # Evaluate policy\n",
    "    policy.eval()\n",
    "\n",
    "    t_reward = []\n",
    "    for _ in range(num_steps):\n",
    "        # Use policy\n",
    "        action, log_prob, value = policy.act(eval_obs)\n",
    "        # print(action)\n",
    "        # Take step in environment\n",
    "        eval_obs, reward, done, info = eval_env.step(action)\n",
    "\n",
    "        t_reward.append(reward)\n",
    "\n",
    "    data_val.append(np.mean(t_reward))\n",
    "    data_train.append(storage.get_reward() / num_envs)\n",
    "    data_steps.append(step)\n",
    "    print(f'Step: {step}\\nMean train reward: {data_train[-1]}\\nMean val reward: {data_val[-1]}')\n",
    "\n",
    "print('Completed training!')\n",
    "try:\n",
    "    torch.save(policy.state_dict, f\"checkpoint.pt\")\n",
    "except:\n",
    "    pass\n",
    "plt.semilogx(data_steps, data_train, label='train', color=\"red\")\n",
    "plt.semilogx(data_steps, data_val, label='validation', color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "\n",
    "for i in range(2000):\n",
    "    name = f\"graph.png\"\n",
    "    if os.path.isfile(os.path.join(os.getcwd(), name)):\n",
    "        continue\n",
    "    else:\n",
    "        plt.savefig(name)\n",
    "        print(\"Graph \", i, \" saved\")\n",
    "        plt.clf()\n",
    "        break\n",
    "\n",
    "with open(f\"sample.csv\", \"w\", encoding=\"utf-8\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"step\", \"train\", \"val\"])\n",
    "    for (s, t, v) in zip(data_train, data_steps, data_val):\n",
    "        writer.writerow([t, s, v])\n",
    "print('Completed training!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you can evaluate the agent and see it plays."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "# Make evaluation environment\n",
    "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels)\n",
    "obs = eval_env.reset()\n",
    "\n",
    "frames = []\n",
    "total_reward = []\n",
    "\n",
    "# Evaluate policy\n",
    "policy.eval()\n",
    "for _ in range(512):\n",
    "\n",
    "  # Use policy\n",
    "  action, log_prob, value = policy.act(obs)\n",
    "\n",
    "\n",
    "  # Take step in environment\n",
    "  obs, reward, done, info = eval_env.step(action)\n",
    "  total_reward.append(torch.Tensor(reward))\n",
    "\n",
    "  # Render environment and store\n",
    "  frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
    "  frames.append(frame)\n",
    "\n",
    "# Calculate average return\n",
    "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
    "print('Average return:', total_reward)\n",
    "\n",
    "# Save frames as video\n",
    "frames = torch.stack(frames)\n",
    "imageio.mimsave('vid.mp4', frames, fps=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}